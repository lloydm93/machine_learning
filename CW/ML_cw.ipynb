{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom Classification Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Research and Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: Dataset\n",
    "Name and Source of the Data: The dataset used for this analysis is the Mushroom Dataset from the UCI Machine Learning Repository.\n",
    "\n",
    "Date and Time of Dataset Acquisition: The dataset was downloaded on [insert date/time here].\n",
    "\n",
    "Reason for Choosing This Dataset: The Mushroom Dataset was chosen due to its comprehensive set of categorical features, making it ideal for classification problems. Additionally, it provides a real-world application of determining the edibility of mushrooms, which has practical significance in the fields of biology and food safety.\n",
    "\n",
    "Dataset Overview: The Mushroom Dataset contains records of different mushroom species, detailing various attributes such as cap shape, cap color, gill size, and odor. Each record is classified as either edible or poisonous. This classification problem serves as an excellent example for applying machine learning techniques to predict the edibility of mushrooms based on their characteristics.\n",
    "\n",
    "Objectives: The main objective of this analysis is to build a predictive model that can classify mushrooms as either edible or poisonous based on their features. This model will utilise various machine learning algorithms to determine the most accurate method for this classification task. The ultimate goal is to develop a reliable model that can aid in the safe identification of mushrooms in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Related Work\n",
    "Title of the Paper: \"Using machine learning on cardiorespiratory fitness data for predicting hypertension: The Henry Ford ExercIse Testing (FIT) Project\" by Sherif Sakr and Radwa Elshawi.\n",
    "\n",
    "Dataset Overview: The paper uses the Henry Ford ExercIse Testing (FIT) Project dataset, which contains information on 23,095 patients who underwent clinician-referred exercise treadmill stress testing at Henry Ford Health Systems between 1991 and 2009. This dataset is relevant to my work as it involves the application of machine learning techniques to predict health outcomes, similar to my objective of predicting mushroom edibility.\n",
    "\n",
    "Objectives and Methods: The paper aims to evaluate and compare the performance of different machine learning techniques for predicting individuals at risk of developing hypertension using cardiorespiratory fitness data. The authors investigated six machine learning techniques: LogitBoost (LB), Bayesian Network classifier (BN), Locally Weighted Naive Bayes (LWB), Artificial Neural Network (ANN), Support Vector Machine (SVM), and Random Tree Forest (RTF). This is relevant to my work as it provides insights into various machine learning methods that can be applied to classification problems.\n",
    "\n",
    "Results: The results show that the Random Tree Forest (RTF) model achieved the best performance with an AUC of 0.93, outperforming all other machine learning techniques examined in the study. This paper is considered significant due to its comprehensive evaluation of multiple machine learning methods and its demonstration of the effectiveness of RTF in predicting hypertension.\n",
    "\n",
    "Evaluation: I believe this paper is excellent as it provides a thorough comparison of different machine learning techniques and highlights the importance of model evaluation. The insights gained from this study can be applied to my project to improve the accuracy of mushroom classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import pointbiserialr\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Mushroom Dataset\n",
    "The dataset, obtained from the UCI Machine Learning Repository, includes various features of mushrooms to determine their edibility. The column names have been manually assigned to represent each feature accurately as per the UCI repository documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to CSV file\n",
    "file_path = 'Mushrooms.data.csv'\n",
    "\n",
    "# Column names of dataset \n",
    "column_names = [\n",
    "    'poisonous', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',\n",
    "    'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', \n",
    "    'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', \n",
    "    'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path, sep=',', names=column_names, header=None)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "df.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Check if there are any missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Save the dataframe for the next steps\n",
    "df.to_csv('explored_mushrooms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "In this section, the dataset is cleaned and pre-processed by converting categorical values to numerical values and handling any missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('explored_mushrooms.csv')\n",
    "\n",
    "# Clean and preprocess the data\n",
    "df['poisonous'] = df['poisonous'].replace({'poisonous': 'p', 'edible': 'e'})\n",
    "df['poisonous'] = df['poisonous'].map({'p': 1, 'e': 0})\n",
    "\n",
    "# Handle missing values by filling with mode\n",
    "df = df.apply(lambda col: col.fillna(col.mode()[0]))\n",
    "\n",
    "# Save the cleaned dataframe\n",
    "df.to_csv('cleaned_mushrooms.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Visualization\n",
    "To understand the class imbalance in the dataset, the distribution of the target variable (poisonous vs. edible) is visualised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('cleaned_mushrooms.csv')\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='poisonous', data=df, hue='poisonous', palette=\"Set2\", legend=False)\n",
    "plt.title('Class Distribution (Edible vs Poisonous)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Feature Distribution Visualization\n",
    "\n",
    "The distribution of several categorical features is plotted to explore how they vary with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the first few categorical features\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, column in enumerate(df.columns[1:5]):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.countplot(x=column, data=df, hue='poisonous', palette=\"Set1\", legend=False)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Visualization\n",
    "\n",
    "The point-biserial correlation for each feature with the target variable (poisonous vs. edible) is computed and the top 10 correlated features are visualised. This helps identify the features most strongly associated with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('cleaned_mushrooms.csv')\n",
    "\n",
    "# Calculate point-biserial correlation for each feature\n",
    "correlations = []\n",
    "for column in df.columns[1:]:  \n",
    "    feature = pd.get_dummies(df[column], drop_first=True) \n",
    "    for col in feature.columns:\n",
    "        corr, _ = pointbiserialr(df['poisonous'], feature[col])\n",
    "        correlations.append((column, col, corr))\n",
    "\n",
    "corr_df = pd.DataFrame(correlations, columns=['Feature', 'Encoded Feature', 'Correlation'])\n",
    "corr_df = corr_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Save top correlated features\n",
    "top_corr_df = corr_df.head(10)\n",
    "top_corr_df.to_csv('top_correlated_features.csv', index=False)\n",
    "\n",
    "# Plot top correlated features\n",
    "plt.figure(figsize=(14, 8)) \n",
    "sns.barplot(x='Correlation', y='Encoded Feature', data=top_corr_df, hue='Encoded Feature', palette='Blues_d', dodge=False, errorbar=None, legend=False)\n",
    "plt.title('Top 10 Point-Biserial Correlations with Target (Poisonous Mushroom)')\n",
    "plt.xlabel('Correlation with Target')\n",
    "plt.ylabel('Encoded Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Features and Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('cleaned_mushrooms.csv')\n",
    "\n",
    "# Convert categorical features to one-hot encoded variables\n",
    "y = df['poisonous']\n",
    "X = pd.get_dummies(df.drop('poisonous', axis=1), drop_first=True)\n",
    "\n",
    "# Convert any boolean columns to integers\n",
    "for col in X.select_dtypes(include='bool').columns:\n",
    "    X[col] = X[col].astype(int)\n",
    "\n",
    "# Verify the encoding\n",
    "print(X.dtypes)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Continue with feature selection and splitting the dataset\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train/test data\n",
    "pd.DataFrame(X_train).to_csv('X_train.csv', index=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Modelling/Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Initial Model\n",
    "An initial classification model is built using Support Vector Machine (SVM) with default parameters stated in the paper. (https://www.semanticscholar.org/reader/dbfc5a911e50b275d9b8adb5d311286d50b3f9dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Reduce the size of the training data\n",
    "X_train_reduced = X_train[:3000]\n",
    "y_train_reduced = y_train[:3000]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_reduced = scaler.fit_transform(X_train_reduced)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the SVM model with adjusted parameters\n",
    "initial_svm_model_adjusted = SVC(kernel='sigmoid', C=50, gamma='auto', random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_adjusted = cross_val_score(initial_svm_model_adjusted, X_train_reduced, y_train_reduced, cv=5)\n",
    "print(f\"Cross-validation scores (Adjusted Model): {cv_scores_adjusted}\")\n",
    "print(f\"Average cross-validation score (Adjusted Model): {np.mean(cv_scores_adjusted)}\")\n",
    "\n",
    "# Train the initial model with adjustments\n",
    "initial_svm_model_adjusted.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "# Make predictions with the initial model with adjustments\n",
    "y_pred_initial_adjusted = initial_svm_model_adjusted.predict(X_test)\n",
    "\n",
    "# Evaluate the initial model with adjustments\n",
    "print(\"Initial Model (Adjusted) - Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_initial_adjusted))\n",
    "print(\"\\nInitial Model (Adjusted) - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_initial_adjusted))\n",
    "print(\"\\nInitial Model (Adjusted) - Accuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred_initial_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Model\n",
    "The model is further improved using cross-validation and parameter tuning with the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel='linear', C=0.1, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Average cross-validation score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Fit the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
